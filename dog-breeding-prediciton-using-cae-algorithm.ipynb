{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2387160,"sourceType":"datasetVersion","datasetId":453611}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports & Environment Setup","metadata":{}},{"cell_type":"code","source":"import os, random, math, numpy as np, pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:12:02.678526Z","iopub.execute_input":"2026-01-30T12:12:02.678963Z","iopub.status.idle":"2026-01-30T12:12:15.554248Z","shell.execute_reply.started":"2026-01-30T12:12:02.678921Z","shell.execute_reply":"2026-01-30T12:12:15.553353Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Set Random Seeds","metadata":{}},{"cell_type":"code","source":"def seed_all(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nseed_all(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:12:15.555754Z","iopub.execute_input":"2026-01-30T12:12:15.556338Z","iopub.status.idle":"2026-01-30T12:12:15.578243Z","shell.execute_reply.started":"2026-01-30T12:12:15.556306Z","shell.execute_reply":"2026-01-30T12:12:15.577103Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Loading Dataset - Dataset Paths (Train / Valid / Test)","metadata":{}},{"cell_type":"code","source":"DATA_ROOT = \"/kaggle/input/70-dog-breedsimage-data-set\"\n\ntrain_dir = os.path.join(DATA_ROOT, \"train\")\nval_dir   = os.path.join(DATA_ROOT, \"valid\")\ntest_dir  = os.path.join(DATA_ROOT, \"test\")\n\nprint(\"Train exists:\", os.path.isdir(train_dir))\nprint(\"Valid exists:\", os.path.isdir(val_dir))\nprint(\"Test exists :\", os.path.isdir(test_dir))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Number of Classes","metadata":{}},{"cell_type":"code","source":"def count_images(root):\n    classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n    counts = []\n    for c in classes:\n        cpath = os.path.join(root, c)\n        n = len([f for f in os.listdir(cpath) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n        counts.append((c, n))\n    df = pd.DataFrame(counts, columns=[\"class\", \"count\"]).sort_values(\"count\", ascending=False)\n    return df, classes\n\ntrain_df, classes = count_images(train_dir)\nval_df, _ = count_images(val_dir)\ntest_df, _ = count_images(test_dir)\n\nprint(\"Num classes:\", len(classes))\nprint(\"Train images:\", train_df[\"count\"].sum())\nprint(\"Val images  :\", val_df[\"count\"].sum())\nprint(\"Test images :\", test_df[\"count\"].sum())\n\ntrain_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Class Distribution: Count Images per Breed","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.hist(train_df[\"count\"], bins=20)\nplt.title(\"Train: Distribution of images per class\")\nplt.xlabel(\"Images per class\")\nplt.ylabel(\"Number of classes\")\nplt.show()\n\ntrain_df.describe()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize Random Training Samples","metadata":{}},{"cell_type":"code","source":"def show_samples(root, classes, n=12):\n    plt.figure(figsize=(12,8))\n    for i in range(n):\n        c = random.choice(classes)\n        cdir = os.path.join(root, c)\n        img_name = random.choice(os.listdir(cdir))\n        img_path = os.path.join(cdir, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n        plt.subplot(3,4,i+1)\n        plt.imshow(img)\n        plt.title(c, fontsize=9)\n        plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\nshow_samples(train_dir, classes, n=12)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Check Image Size Variations","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ndef sample_sizes(root, classes, k=300):\n    sizes = []\n    for _ in range(k):\n        c = random.choice(classes)\n        cdir = os.path.join(root, c)\n        img_name = random.choice(os.listdir(cdir))\n        img_path = os.path.join(cdir, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n        sizes.append(img.size)\n    return Counter(sizes).most_common(10)\n\nsample_sizes(train_dir, classes, k=300)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Compute Dataset Mean & Std for Normalization","metadata":{}},{"cell_type":"code","source":"def compute_mean_std(dataset, num_batches=50, batch_size=64):\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    mean = 0.\n    var  = 0.\n    n = 0\n    for i, (x, _) in enumerate(loader):\n        if i >= num_batches: break\n        x = x.to(device)\n        b = x.size(0)\n        x = x.view(b, x.size(1), -1)\n        mean += x.mean(dim=2).sum(dim=0)\n        var  += x.var(dim=2, unbiased=False).sum(dim=0)\n        n += b\n    mean /= n\n    var  /= n\n    std = torch.sqrt(var)\n    return mean.detach().cpu().numpy(), std.detach().cpu().numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stats_tf = transforms.Compose([transforms.ToTensor()])\ntmp_ds = datasets.ImageFolder(train_dir, transform=stats_tf)\nmean, std = compute_mean_std(tmp_ds)\nmean, std","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation & Preprocessing Pipelines","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = 256\nBATCH_SIZE = 64\n\ntrain_tf = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.RandomAffine(0, translate=(0.05, 0.05)),\n    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std),\n    transforms.RandomErasing(p=0.25, scale=(0.02, 0.12), ratio=(0.3, 3.3)),\n])\n\neval_tf = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.CenterCrop(IMG_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std),\n])\n\nfrom torchvision.datasets import ImageFolder\nimport re\n\ndef canon(s: str) -> str:\n    return re.sub(r\"\\s+\", \" \", s.strip()).lower()\n\nclass ImageFolderWithCanonicalMap(ImageFolder):\n    def __init__(self, root, train_class_to_idx, transform=None):\n        super().__init__(root=root, transform=transform)\n        train_c2i = {canon(k): v for k, v in train_class_to_idx.items()}\n        fixed_samples = []\n        missing = set()\n        for path, local_target in self.samples:\n            class_name = self.classes[local_target]\n            key = canon(class_name)\n            if key not in train_c2i:\n                missing.add(class_name)\n                continue\n            fixed_samples.append((path, train_c2i[key]))\n        if missing:\n            print(\"WARNING: unmatched classes:\", list(sorted(missing))[:10], \"...\")\n        self.samples = fixed_samples\n        self.targets = [t for _, t in self.samples]\n\ntrain_ds = ImageFolder(train_dir, transform=train_tf)\nval_ds   = ImageFolderWithCanonicalMap(val_dir,  train_ds.class_to_idx, transform=eval_tf)\ntest_ds  = ImageFolderWithCanonicalMap(test_dir, train_ds.class_to_idx, transform=eval_tf)\n\nnum_classes = len(train_ds.classes)\nprint(\"Classes:\", num_classes)\nprint(\"Val samples:\", len(val_ds))\nprint(\"Test samples:\", len(test_ds))\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Class Weights Compute","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ncounts = Counter(train_ds.targets)\n\nclass_weights = torch.zeros(num_classes)\nfor i in range(num_classes):\n    class_weights[i] = counts[i]\n\n# inverse frequency weighting\nclass_weights = class_weights.sum() / class_weights\nclass_weights = class_weights / class_weights.mean()\n\nclass_weights = class_weights.to(device)\nprint(\"Class weights computed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ReconWrapper(torch.utils.data.Dataset):\n    def __init__(self, base_ds):\n        self.base = base_ds\n    def __len__(self):\n        return len(self.base)\n    def __getitem__(self, idx):\n        x, _ = self.base[idx]\n        return x, x  # input, target = same image\n\ntrain_recon_loader = DataLoader(ReconWrapper(train_ds), batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_recon_loader   = DataLoader(ReconWrapper(val_ds),   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convolutional Autoencoder (CAE) Architecture","metadata":{}},{"cell_type":"code","source":"class CAE(nn.Module):\n    def __init__(self, latent_dim=256, img_size=224):\n        super().__init__()\n\n        self.enc = nn.Sequential(\n            nn.Conv2d(3, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(),     # /2\n            nn.Conv2d(32, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),    # /4\n            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),  # /8\n            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(), # /16\n            nn.Conv2d(256, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(), # /32\n        )\n\n        # compute encoder output shape dynamically\n        with torch.no_grad():\n            dummy = torch.zeros(1, 3, img_size, img_size)\n            h = self.enc(dummy)\n            self.enc_shape = h.shape[1:]                # (C,H,W)\n            self.flat_dim = h.numel()                   # C*H*W\n\n        self.to_latent = nn.Linear(self.flat_dim, latent_dim)\n        self.from_latent = nn.Linear(latent_dim, self.flat_dim)\n\n        self.dec = nn.Sequential(\n            nn.ConvTranspose2d(256, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(),\n            nn.ConvTranspose2d(32, 3, 4, 2, 1),\n        )\n\n    def encode(self, x):\n        x = self.enc(x)\n        x = x.flatten(1)\n        return self.to_latent(x)\n\n    def decode(self, z):\n        x = self.from_latent(z).view(-1, *self.enc_shape)\n        return self.dec(x)\n\n    def forward(self, x):\n        z = self.encode(x)\n        return self.decode(z)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CAE Evaluation Function (Reconstruction Loss)","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_recon(model, loader):\n    model.eval()\n    total_loss, n = 0.0, 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        x_hat = model(x)\n        loss = F.mse_loss(x_hat, y)\n        total_loss += loss.item() * x.size(0)\n        n += x.size(0)\n    return total_loss / n\n\nLATENT_DIM = 512\ncae = CAE(latent_dim=LATENT_DIM, img_size=IMG_SIZE).to(device)\n\nopt_ae = torch.optim.AdamW(cae.parameters(), lr=3e-4, weight_decay=1e-4)\n\n\nAE_EPOCHS = 40\nbest_val_recon = 1e9\n\nfor ep in range(1, AE_EPOCHS+1):\n    cae.train()\n    total_loss, n = 0.0, 0\n    for x, y in train_recon_loader:\n        x, y = x.to(device), y.to(device)\n        opt_ae.zero_grad(set_to_none=True)\n        x_hat = cae(x)\n        loss = F.mse_loss(x_hat, y)\n        loss.backward()\n        opt_ae.step()\n        total_loss += loss.item() * x.size(0)\n        n += x.size(0)\n\n    train_loss = total_loss / n\n    val_loss = eval_recon(cae, val_recon_loader)\n    print(f\"[CAE] Epoch {ep:02d} | Train Recon Loss: {train_loss:.5f} | Val Recon Loss: {val_loss:.5f}\")\n\n    if val_loss < best_val_recon:\n        best_val_recon = val_loss\n        torch.save(cae.state_dict(), \"best_cae.pth\")\n\nprint(\"Best Val Recon Loss:\", best_val_recon)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"cae.load_state_dict(torch.load(\"best_cae.pth\", map_location=device))\ncae.eval()\n\nx, _ = next(iter(val_recon_loader))\nx = x.to(device)[:8]\nwith torch.no_grad():\n    x_hat = cae(x)\n\n# unnormalize for viewing\ndef unnorm(t):\n    m = torch.tensor(mean, device=t.device).view(1,3,1,1)\n    s = torch.tensor(std, device=t.device).view(1,3,1,1)\n    return (t*s + m).clamp(0,1)\n\nx_vis = unnorm(x).cpu()\nxh_vis = unnorm(x_hat).cpu().clamp(0,1)\n\nplt.figure(figsize=(12,4))\nfor i in range(8):\n    plt.subplot(2,8,i+1)\n    plt.imshow(x_vis[i].permute(1,2,0))\n    plt.axis(\"off\")\n    plt.subplot(2,8,8+i+1)\n    plt.imshow(xh_vis[i].permute(1,2,0))\n    plt.axis(\"off\")\nplt.suptitle(\"Top: Original | Bottom: Reconstructed (CAE)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}